{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Resnet34.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPzwRSr4uFLxt8gIwjAV2FG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"W_-XdjdduupL"},"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import img_to_array, array_to_img, ImageDataGenerator\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from keras.regularizers import l2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJJ6OsgJuyTs"},"source":["from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, BatchNormalization, Add, Activation, AveragePooling2D, Dropout, LeakyReLU\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import Adam, SGD\n","from tensorflow.keras.losses import categorical_crossentropy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgYxZCOLu0NX"},"source":["def load_dataset(dataset):\n","  if dataset == 'mnist':\n","    return tf.keras.datasets.mnist.load_data()\n","  elif dataset == 'fashion_mnist':\n","    return tf.keras.datasets.fashion_mnist.load_data()\n","  elif dataset == 'cifar-10':\n","    return tf.keras.datasets.cifar10.load_data()\n","  else:\n","    return -1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6AWH566Ju4u7"},"source":["def preprocess_dataset(x_train, y_train, x_test, y_test):\n","  # expand the dimension (add the channel axis to MNIST and Fashion_MNIST)\n","  if len(x_train.shape) == 3 :\n","    x_train = tf.expand_dims(x_train, -1)\n","    x_test = tf.expand_dims(x_test, -1)\n","\n","  # resize the image\n","  x_train = np.asarray([img_to_array(array_to_img(im, scale=False).resize((64,64))) for im in x_train])\n","  x_test = np.asarray([img_to_array(array_to_img(im, scale=False).resize((64,64))) for im in x_test])\n","\n","  # Convert the labels to their one-hot representation\n","  y_train = to_categorical(y_train)\n","  y_test = to_categorical(y_test)\n","\n","  # Normalise the dataset by mean subtraction\n","  x_train = x_train.astype('float32')\n","  x_test = x_test.astype('float32')\n","  x_train_mean = np.mean(x_train, axis=0)\n","  x_train -= x_train_mean\n","  x_test -= x_train_mean\n","\n","  return x_train, y_train, x_test, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qo9628OBu8Dl"},"source":["def visualize(pred_labels, test_labels, test_images, dataset_name):\n","  # list to store the index of the success cases and fail cases\n","  success_cases = []\n","  fail_cases = []\n","  for i in range(len(test_labels)):\n","    if test_labels[i].argmax() == pred_labels[i].argmax() and len(success_cases)<5:\n","      success_cases.append(i)\n","    elif test_labels[i].argmax() != pred_labels[i].argmax() and len(fail_cases)<5:\n","      fail_cases.append(i)\n","    if len(success_cases) == 4 and len(fail_cases) == 4:\n","      break\n","  \n","  # cmap = gray from MNIST and fashion_mnist\n","  if len(test_images[0].shape) == 2:\n","    cmap = 'gray'\n","  else:\n","    cmap = None\n","  \n","  # plotting the success cases\n","  fig, ax = plt.subplots(nrows=2, ncols=2)\n","  for row in ax:\n","    for col in row:\n","        col.imshow(test_images[success_cases.pop(0)], interpolation='nearest', cmap=cmap)\n","  fig.suptitle('Success Cases of resnet34 on ' + dataset_name)\n","\n","  # plotting the fail cases\n","  plt.show()\n","  fig, ax = plt.subplots(nrows=2, ncols=2)\n","  for row in ax:\n","    for col in row:\n","        col.imshow(test_images[fail_cases.pop(0)], interpolation='nearest', cmap=cmap)\n","  fig.suptitle('Fail Cases of resnet34 on ' + dataset_name)\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fllfQA0xvAt1"},"source":["def identity_block(x, f, dropout_rate):\n","  x_skip = x\n","  x = Conv2D(filters=f,kernel_size=(3,3),strides=(1, 1),padding=\"same\", kernel_initializer=tf.initializers.GlorotUniform(),kernel_regularizer=l2(1e-4))(x)\n","  x = Dropout(dropout_rate)(x)\n","  x = BatchNormalization(axis=3)(x)\n","  # x = Activation('relu')(x)\n","  x = LeakyReLU()(x)\n","  x = Conv2D(filters=f,kernel_size=(3,3),strides=(1, 1),padding=\"same\", kernel_initializer=tf.initializers.GlorotUniform(),kernel_regularizer=l2(1e-4))(x)\n","  x = Dropout(dropout_rate)(x)\n","  x = BatchNormalization(axis=3)(x)\n","  x = Add()([x, x_skip])\n","  # x = Activation('relu')(x)\n","  x = LeakyReLU()(x)\n","  return x\n","\n","def conv_block(x, f, dropout_rate):\n","  x_skip = x\n","  x = Conv2D(filters=f,kernel_size=(3,3),strides=(2, 2),padding=\"same\", kernel_initializer=tf.initializers.GlorotUniform(),kernel_regularizer=l2(1e-4))(x)\n","  x = Dropout(dropout_rate)(x)\n","  x = BatchNormalization(axis=3)(x)\n","  # x = Activation('relu')(x)\n","  x = LeakyReLU()(x)\n","  x = Conv2D(filters=f,kernel_size=(3,3),strides=(1, 1),padding=\"same\", kernel_initializer=tf.initializers.GlorotUniform(),kernel_regularizer=l2(1e-4))(x)\n","  x = Dropout(dropout_rate)(x)\n","  x = BatchNormalization(axis=3)(x)\n","  x_skip = Conv2D(filters=f,kernel_size=(1,1),strides=(2, 2),padding=\"same\", kernel_initializer=tf.initializers.GlorotUniform(),kernel_regularizer=l2(1e-4))(x_skip)\n","  x = Dropout(dropout_rate)(x)\n","  x_skip = BatchNormalization(axis=3)(x_skip)\n","  x = Add()([x, x_skip])\n","  # x = Activation('relu')(x)\n","  x = LeakyReLU()(x)\n","  return x\n","\n","def model_resnet34(image_shape, num_category):\n","  filters = [64, 128, 256, 512]\n","  blocks = [3, 4, 6, 3]\n","  dropout_rate = 0.2\n","  inputs = Input(shape=image_shape)\n","  x = Conv2D(filters=64,kernel_size=(7,7),strides=(2, 2),padding=\"same\",kernel_initializer=tf.initializers.GlorotUniform(),kernel_regularizer=l2(1e-4))(inputs)\n","  x = Dropout(dropout_rate)(x)\n","  x = BatchNormalization(axis=3)(x)\n","  # x = Activation('relu')(x)\n","  x = LeakyReLU()(x)\n","  x = MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n","\n","  first_block = True\n","  for b in blocks:\n","    f = filters.pop(0)\n","    x = identity_block(x, f, dropout_rate) if first_block == True else conv_block(x, f, dropout_rate)\n","    first_block = False\n","    for _ in range(b-1):\n","      x = identity_block(x, f, dropout_rate)\n","\n","  x = AveragePooling2D(pool_size=(2,2))(x)\n","  x= Flatten()(x)\n","  outputs = Dense(units=num_category, activation='softmax', kernel_initializer=tf.initializers.GlorotUniform())(x)\n","  model = Model(inputs=inputs, outputs=outputs)\n","  \n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FTsqhVonvBaZ"},"source":["dataset_name = 'mnist'\n","(train_img, train_lbl), (test_img, test_lbl) = load_dataset(dataset_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xlE3LKE2vGeZ"},"source":["train_images, train_labels, test_images, test_labels = preprocess_dataset(train_img, train_lbl, test_img, test_lbl)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B8HMccZIvJRU"},"source":["augment_data = True\n","batch_size = 128\n","validation_percent = 0.1\n","epochs = 100\n","if augment_data:\n","  datagen = ImageDataGenerator(\n","      width_shift_range=0.1,\n","      height_shift_range=0.1,\n","      horizontal_flip=True,\n","      rotation_range=0,\n","      fill_mode='nearest',\n","      validation_split=validation_percent\n","  )\n","  train_iterator = datagen.flow(train_images, train_labels, batch_size=batch_size, subset='training')\n","  validation_iterator = datagen.flow(train_images, train_labels, batch_size=batch_size, subset='validation')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Q1LJw-fvMQr"},"source":["resnet34 = model_resnet34(image_shape = train_images[0].shape, num_category = len(train_labels[0]))\n","resnet34.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n","resnet34.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F3rGD1wYvQX2"},"source":["reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), patience=3, min_lr=0.5e-6)\n","early_stop = EarlyStopping(monitor='val_loss', patience=15, verbose=1, restore_best_weights=True)\n","if augment_data:\n","  history = resnet34.fit(train_iterator, epochs=epochs, steps_per_epoch=len(train_iterator), \n","                                validation_data=validation_iterator, validation_steps=len(validation_iterator), \n","                                callbacks=[reduce_lr, early_stop])\n","else:\n","  history = resnet34.fit(x=train_images, y=train_labels, epochs=epochs, batch_size=batch_size, \n","                      validation_split=validation_percent,\n","                      callbacks=[reduce_lr, early_stop])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OvEofETGvRH_"},"source":["# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy of resnet34 on ' + dataset_name)\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss of resnet34 on ' + dataset_name)\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-0OzfhuJvax0"},"source":["eval = resnet34.evaluate(test_images, test_labels, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aQciULXqv0Gm"},"source":["predictions = resnet34.predict(test_images, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BF_--wJwv2UQ"},"source":["visualize(predictions, test_labels, test_img, dataset_name)"],"execution_count":null,"outputs":[]}]}